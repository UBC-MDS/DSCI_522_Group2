{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "0.396"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "highest_importance_coeff"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "'0.847 (+/- 0.000)'"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "optimized_knn_test_score"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "'0.852 (+/- 0.001)'"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "dt_test_score"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "11179"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "false_negative"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "784"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "false_negative"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables with glue here\n",
    "feature_importance_df = pd.read_csv(\"../results/tables/feature_importances.csv\").round(3)\n",
    "model_comparison_df = pd.read_csv(\"../results/tables/model_comparison_results.csv\").round(3)\n",
    "confusion_matrix_dt = pd.read_csv(\"../results/tables/confusion_matrix_DT.csv\")\n",
    "glue(\"highest_importance_coeff\", feature_importance_df.iloc[4,2],display=False)\n",
    "glue(\"optimized_knn_test_score\", model_comparison_df.iloc[3,3],display=False)\n",
    "glue(\"dt_test_score\", model_comparison_df.iloc[1,3],display=False)\n",
    "glue(\"false_negative\", confusion_matrix_dt.iloc[1,1],display=False)\n",
    "glue(\"false_positive\", confusion_matrix_dt.iloc[0,2],display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This project endeavors to develop a predictive classification model for ascertaining an individual's diabetic status, while comparing the efficiency of optimized decision tree and optimized k-nearest neighbours (k-nn) algorithms. The dataset used in this analysis is collected through the Behavioral Risk Factor Surveillance System (BFRSS) by the Centers for Disease Control and Prevention (CDC) for the year 2015. Notably, the primary determinant influencing the prediction is identified as the feature Body Mass Index (BMI), displaying a coefficient of {glue:text}`highest_importance_coeff`  as revealed by the logistic regression model. The optimized decision tree model demonstrates a test score of {glue:text}`dt_test_score`,while the optimized k-nn model yields a test score of {glue:text}`optimized_knn_test_score`. Both of the test scores are relatively close to the validation score which shows that the model will generalized well to unseen data, however, there is still room for improvement in the test score. \n",
    "\n",
    "Our final classsifer is chosen to be the Desicion Tree model as it has both the highest test and train score in comparison with the optimized k-nn model. However, from the confusion matrix, we can see that our model has made {glue:text}`false_negative` type II error, which predicted the individual as non-diabetic when they are actually diabetic. On the other hand, it has also made {glue:text}`false_positive` type I error, which predicted the individual as diabetic when they are actually non-diabetic. In our case, both type II error and type I error is equivalently important as injecting insulin in a non-diabetic individual can be fatal and vice versa. Hence, we chose f1 score as our scoring matrix which is a hormonic balance between both type I and type II errors. From this result, we can also see that there is a huge class imbalance in between diabetic and non-diabetic class, which will be taken into account in the preprocessor in the next version of this analysis.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Diabetes mellitus, commonly referred to as diabetes is a disease which impacts the bodyâ€™s control of blood glucose levels (Sapra, Bhandari 2023). It is important to note that there are different types of diabetes, although we do not explore this discrepancy in this project (Sapra, Bhandari 2023). Diabetes is a manageable disease thanks to the discovery of insulin in 1922. Globally, 1 in 11 adults have diabetes (Sapra, Bhandari 2023). As such, understanding the factors which are strongly related to diabetes can be important for researchers studying how to better prevent or manage the disease. In this project, we create several machine learning models to predict diabetes in a patient and evaluate the success of these models. We also explore the coefficients of a logistic regression model to better understand the factors which are associated with diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Data\n",
    "The dataset used in this project is a collection of the Centers for Disease Control and Prevention (CDC) diabetes health indicators collected as a response to the CDC's BRFSS2015 survey. The data were sourced from the UCI Machine Learning Repository (Burrows, Hora, Geiss, Gregg, and Albright 2017) which can be found [here](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators). The file specifically used from this repository for this analysis includes 70, 692 survey responses from which 50% of the respondents recorded having either prediabetes or diabetes. Each row in the dataset represents a recorded survey response including whether or not the responded has diabetes or prediabetes, and a collection of 21 other diabetes health indicators identified by the CDC. \n",
    "\n",
    "### Analysis\n",
    "Four separate classification models were tested on this dataset with the purpose of determining the best model for classifying a patient with diabetes or prediabetes as opposed to no diabetes or prediabetes. The classifiers tested were: dummy, decision tree, k-nearest neighbors (k-nn), and logistic regression. All features from the original dataset were included in each model. In all cases, the data were split into training and testing datasets, with 80% of the data designated as training and 20% as testing. The data was preprocessesed such that all continuous (non-binary) variables were scaled using a scikit-learn's StandardScaler function. Model performance was tested using a 10 - fold cross validation score. Feature importance was investigated using the coefficients generated by the logistic regression algorithm. The k-nn algorithm's hyperparameter K was optimized using the F1 score as the classification metric. Python programming (Van Rossum and Drake 2009) was used for all analysis. The following Python packages were used for this analysis: Pandas (McKinney 2010), altair (VanderPlas, 2018), and scikit-learn (Pedregosa et al. 2011).\n",
    "\n",
    "### Results & Discussion\n",
    "\n",
    "Before we  begin building our models for comparison, we plotted the histogram distribution of each feature in the dataset to have an insight on the features. used with 0 and 1 representing the non-diabetic class(blue) and the diabetic class(orange) respectively  {numref}`Figure {number} <feature_histogram_by_class.png>`. It is seen that for all the features, there is a significant overlapping in between the classes as most of the features are binary features. However, their max and mean values for each feature seems to be different, thus, we decided to include all the features in our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/feature_histogram_by_class.png\n",
    "---\n",
    "width: 800px\n",
    "name: feature_histogram_by_class\n",
    "---\n",
    "Histogram distribution for each feature in the training data for both non-diabetic (0 & blue) and diabetic (1 & orange) class.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### References\n",
    "\n",
    "{bibliography} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "571",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
