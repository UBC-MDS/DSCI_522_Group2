{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_score</th>\n      <th>train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>knn</td>\n      <td>0.040 (+/- 0.004)</td>\n      <td>0.763 (+/- 0.091)</td>\n      <td>0.846 (+/- 0.000)</td>\n      <td>0.846 (+/- 0.000)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>decision tree</td>\n      <td>0.155 (+/- 0.005)</td>\n      <td>0.010 (+/- 0.001)</td>\n      <td>0.851 (+/- 0.003)</td>\n      <td>0.851 (+/- 0.001)</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "      Unnamed: 0           fit_time         score_time         test_score  \\\n0            knn  0.040 (+/- 0.004)  0.763 (+/- 0.091)  0.846 (+/- 0.000)   \n1  decision tree  0.155 (+/- 0.005)  0.010 (+/- 0.001)  0.851 (+/- 0.003)   \n\n         train_score  \n0  0.846 (+/- 0.000)  \n1  0.851 (+/- 0.001)  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "knn_tree_cross_val"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define variables with glue here\n",
    "scores_df = pd.read_csv(\"../results/tables/model_comparison.csv\")\n",
    "glue(\"dt_test_score\", scores_df.loc['Decision tree', 'test_score']) \n",
    "glue(\"lr_test_score\", scores_df.loc['Logistic regression', 'test_score'])\n",
    "glue(\"dummy_score\", scores_df.loc['Dummy', 'test_score'])\n",
    "glue(\"model_score_table\", scores_df, display=False)\n",
    "\n",
    "coef_df = pd.read_csv(\"../results/tables/model_comparison.csv\").round(2).sort_values(by='coefficients', ascending=False)\n",
    "glue(\"coef_table\", coef_df, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This project endeavors to develop a predictive classification model for ascertaining an individual's diabetic status, while comparing the efficiency of logistic regression and k-nearest neighbours (k-nn) algorithms. The dataset used in this analysis is collected through the Behavioral Risk Factor Surveillance System (BFRSS) by the Centers for Disease Control and Prevention (CDC) for the year 2015. Notably, the primary determinant influencing the prediction is identified as the feature High Blood Pressue (HighBP), displaying a coefficient of 0.354 as revealed by the logistic regression model. On top of considering a logisitc regression model, we also explored a k-nearest nieghbours model and a decision tree model for predicting diabetes. The optimized logistic regression model demonstrates a test score of 0.728 ,while the k-nn model yields a test score of 0.746. Both of the test scores are relatively close to the validation score which shows that the model will generalized well to unseen data, however, there is still room for improvement in the test score. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Diabetes mellitus, commonly referred to as diabetes is a disease which impacts the bodyâ€™s control of blood glucose levels (Sapra, Bhandari 2023). It is important to note that there are different types of diabetes, although we do not explore this discrepancy in this project (Sapra, Bhandari 2023). Diabetes is a manageable disease thanks to the discovery of insulin in 1922. Globally, 1 in 11 adults have diabetes (Sapra, Bhandari 2023). As such, understanding the factors which are strongly related to diabetes can be important for researchers studying how to better prevent or manage the disease. In this project, we create several machine learning models to predict diabetes in a patient and evaluate the success of these models. We also explore the coefficients of a logistic regression model to better understand the factors which are associated with diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Data\n",
    "The dataset used in this project is a collection of the Centers for Disease Control and Prevention (CDC) diabetes health indicators collected as a response to the CDC's BRFSS2015 survey. The data were sourced from the UCI Machine Learning Repository (Burrows, Hora, Geiss, Gregg, and Albright 2017) which can be found [here](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators). The file specifically used from this repository for this analysis includes 70, 692 survey responses from which 50% of the respondents recorded having either prediabetes or diabetes. Each row in the dataset represents a recorded survey response including whether or not the responded has diabetes or prediabetes, and a collection of 21 other diabetes health indicators identified by the CDC. \n",
    "\n",
    "### Analysis\n",
    "In our efforts to determine the best model for classifying a patient with diabetes or prediabetes as opposed to no diabetes or prediabetes, we performed hyperparamter optimization on both a knn model and a decision tree model. We also explored a logistic regression model to gain insight into which features may contribute most to a classification of diabetes. All features from the original dataset were included in each model. In all cases, the data were split into training and testing datasets, with 80% of the data designated as training and 20% as testing. The data was preprocessesed such that all continuous (non-binary) variables were scaled using a scikit-learn's StandardScaler function. Model performance was tested using a 10 - fold cross validation score. Feature importance was investigated using the coefficients generated by the logistic regression algorithm. The k-nn algorithm's hyperparameter K was optimized using the F1 score as the classification metric. Python programming (Van Rossum and Drake 2009) was used for all analysis. The following Python packages were used for this analysis: Pandas (McKinney 2010), altair (VanderPlas, 2018), and scikit-learn (Pedregosa et al. 2011).\n",
    "\n",
    "### Results & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through using a random search we performed hyper parameter optimization on a knn model and a decision tree model. \n",
    "The table below, ({numref}`Figure {number} <knn_tree_cross_val>`) shows the results of performing cross validation on the best performing knn and decision tree models, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compared the two optimized models to a dummy classifier and a logistic regression model. The optimized decision tree model out-performed all others, with a cross validation score of {glue:text} dt_test_score. As seen in the table below  ({numref}`Figure {number} <model_score_table>`), all of the scores were quite high for all models, even the dummy model. This is likely due to the imbalance in the target features, which should be addressed if this model were to be improved and used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ":figwidth: 400px\n",
    ":name: \"model_score_table\"\n",
    "\n",
    "Cross validation results for all optimized models, as a result of a random search hyperparamater optimization, compared to a dummy classifier.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the logistic regression model did not have the best performance compared to the other models, with a cross-validation score of {glue:text} {glue:text}lr_test_score its coefficients were still useful in investigating which features were weighted as more important in indicating whether a person has diabetes/prediabetes. The results, shown in the table below ({numref}`Figure {number} <coef_table>`), indicate that general health (GenHlth), is the largest predictor of diabetes/prediabetes, followed by body mass index (BMI), high blood pressure (HighBP), and age, respectively. All of these factors, unsurprisingly had a positive correlation with the presence of diabetes/prediabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ":figwidth: 200px\n",
    ":name: \"coef_table\"\n",
    "\n",
    "Logistic regression coefficients by feature.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "{bibliography}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
